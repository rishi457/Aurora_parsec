{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdGsRmm4V6Is"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_and_preprocess_data(solar_wind_path, sunspots_path, labels_path, num_entries=100000):\n",
        "    # Load the first 1000 entries from each CSV file\n",
        "    solar_wind_data = pd.read_csv(solar_wind_path, nrows=num_entries)\n",
        "    sunspots_data = pd.read_csv(sunspots_path, nrows=num_entries)\n",
        "    labels_data = pd.read_csv(labels_path, nrows=num_entries)\n",
        "\n",
        "    # Convert timedelta to separate columns for days and time\n",
        "    def convert_timedelta(df):\n",
        "        df['timedelta'] = pd.to_timedelta(df['timedelta'])\n",
        "        df['days'] = df['timedelta'].dt.days\n",
        "        df['time'] = df['timedelta'] - pd.to_timedelta(df['days'], unit='d')\n",
        "        return df\n",
        "\n",
        "    # Apply the function to each dataset\n",
        "    solar_wind_data = convert_timedelta(solar_wind_data)\n",
        "    sunspots_data = convert_timedelta(sunspots_data)\n",
        "    labels_data = convert_timedelta(labels_data)\n",
        "\n",
        "    # Merge data based on timestamps\n",
        "    merged_data = pd.merge(solar_wind_data, sunspots_data, on='days', how='inner')\n",
        "    merged_data = pd.merge(merged_data, labels_data, on='days', how='inner')\n",
        "\n",
        "    # Feature selection\n",
        "    selected_features = ['bx_gse', 'by_gse', 'bz_gse', 'theta_gse','phi_gse', 'bx_gsm', 'by_gsm', 'bz_gsm', 'theta_gsm', 'phi_gsm', 'bt','density', 'speed', 'temperature', 'source']\n",
        "\n",
        "    # Extract features and target variable\n",
        "    X = merged_data[selected_features]\n",
        "    y = merged_data['dst']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Specify file paths\n",
        "solar_wind_path = '/content/drive/MyDrive/aurora/Train_on_this/solar_wind.csv'\n",
        "sunspots_path = '/content/drive/MyDrive/aurora/Train_on_this/sunspots_smooth.csv'\n",
        "labels_path = '/content/drive/MyDrive/aurora/Train_on_this/labels(dst).csv'\n",
        "\n",
        "# Load and preprocess data with 1000 entries\n",
        "X, y = load_and_preprocess_data(solar_wind_path, sunspots_path, labels_path)\n",
        "\n",
        "# Exclude non-numeric columns\n",
        "numeric_columns = X.select_dtypes(include=['number']).columns\n",
        "X_numeric = X[numeric_columns]\n",
        "\n",
        "# Preprocessing\n",
        "scaler = StandardScaler()\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_scaled = scaler.fit_transform(imputer.fit_transform(X_numeric))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check if the resulting train set is not empty\n",
        "if len(X_train) > 0:\n",
        "    # Define a simple neural network model\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(1)  # Output layer for regression\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save('/content/drive/MyDrive/aurora/trained_model1.h5')\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    predictions = model.predict(X_test).flatten()\n",
        "\n",
        "    # Evaluate the model\n",
        "    rmse = mean_squared_error(y_test, predictions, squared=False)\n",
        "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "else:\n",
        "    print(\"Error: Empty training set.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to load and preprocess data\n",
        "def load_and_preprocess_data1(solar_wind_path, sunspots_path,  num_entries=1000000):\n",
        "    # Load the first 1000 entries from each CSV file\n",
        "    solar_wind_data = pd.read_csv(solar_wind_path, nrows=num_entries)\n",
        "    sunspots_data = pd.read_csv(sunspots_path, nrows=num_entries)\n",
        "\n",
        "    # Convert timedelta to separate columns for days and time\n",
        "    def convert_timedelta(df):\n",
        "        df['timedelta'] = pd.to_timedelta(df['timedelta'])\n",
        "        df['days'] = df['timedelta'].dt.days\n",
        "        df['time'] = df['timedelta'] - pd.to_timedelta(df['days'], unit='d')\n",
        "        return df\n",
        "\n",
        "    # Apply the function to each dataset\n",
        "    solar_wind_data = convert_timedelta(solar_wind_data)\n",
        "    sunspots_data = convert_timedelta(sunspots_data)\n",
        "\n",
        "    # Merge data based on timestamps\n",
        "    merged_data = pd.merge(solar_wind_data, sunspots_data, on='days', how='inner')\n",
        "\n",
        "    # Feature selection\n",
        "    selected_features = ['bx_gse', 'by_gse', 'bz_gse', 'theta_gse','phi_gse', 'bx_gsm', 'by_gsm', 'bz_gsm', 'theta_gsm', 'phi_gsm', 'bt','density', 'speed', 'temperature', 'source']\n",
        "\n",
        "    # Extract features and target variable\n",
        "    X = merged_data[selected_features]\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "G5OrOEeHWCwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "solar_wind_path = '/content/drive/MyDrive/aurora/Dont_Train_on_this/solar_wind.csv'\n",
        "sunspots_path = '/content/drive/MyDrive/aurora/Dont_Train_on_this/sunspots_smooth.csv'\n",
        "\n",
        "# Load and preprocess data with 100000 entries\n",
        "X = load_and_preprocess_data1(solar_wind_path, sunspots_path, num_entries=100000)\n",
        "\n",
        "# Exclude non-numeric columns\n",
        "numeric_columns = X.select_dtypes(include=['number']).columns\n",
        "X_numeric = X[numeric_columns]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_scaled = scaler.fit_transform(imputer.fit_transform(X_numeric))\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/aurora/trained_model.h5')\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_scaled).flatten()\n",
        "\n",
        "# Create a DataFrame with predictions\n",
        "predictions_df = pd.DataFrame({'Predictions': predictions}, index=X.index)\n",
        "\n",
        "# Save predictions to a new file\n",
        "predictions_df.to_csv('/content/drive/MyDrive/aurora/predictions.csv')"
      ],
      "metadata": {
        "id": "gLoT5wzlWIGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}